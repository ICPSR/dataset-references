{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy NER\n",
    "\n",
    "NER pipeline to detect `Dataset` entities. Training and evaluation annotations made using [data-to-prodigy](https://prodi.gy/docs/recipes#data-to-spacy) recipe in Prodigy CLI by loading .jsonl labels into Prodigy database.\n",
    "\n",
    "## Sources:\n",
    "- bibliofake_20220309_v4 (2,005 labels)\n",
    "- s2orc_20220309_v4 (2,005 labels)\n",
    "- paperpile_20220309_v4 (2,004 labels)\n",
    "\n",
    "NER inputs (80/20 split)\n",
    "- 4812 training examples (ner/train.spacy)\n",
    "- 1202 evaluation examples (ner/dev.spacy)\n",
    "- custom configuration file (/config.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "print(spacy.__version__)\n",
    "print(spacy.require_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[paths]\n",
      "train = \"../ner/train.spacy\"\n",
      "dev = \"../ner/dev.spacy\"\n",
      "vectors = null\n",
      "init_tok2vec = null\n",
      "\n",
      "[system]\n",
      "gpu_allocator = \"pytorch\"\n",
      "seed = 0\n",
      "\n",
      "[nlp]\n",
      "lang = \"en\"\n",
      "pipeline = [\"transformer\",\"ner\"]\n",
      "batch_size = 128\n",
      "disabled = []\n",
      "before_creation = null\n",
      "after_creation = null\n",
      "after_pipeline_creation = null\n",
      "tokenizer = {\"@tokenizers\":\"spacy.Tokenizer.v1\"}\n",
      "\n",
      "[components]\n",
      "\n",
      "[components.ner]\n",
      "factory = \"ner\"\n",
      "incorrect_spans_key = null\n",
      "moves = null\n",
      "update_with_oracle_cut_size = 100\n",
      "\n",
      "[components.ner.model]\n",
      "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
      "state_type = \"ner\"\n",
      "extra_state_tokens = false\n",
      "hidden_width = 64\n",
      "maxout_pieces = 2\n",
      "use_upper = false\n",
      "nO = null\n",
      "\n",
      "[components.ner.model.tok2vec]\n",
      "@architectures = \"spacy-transformers.TransformerListener.v1\"\n",
      "grad_factor = 1.0\n",
      "pooling = {\"@layers\":\"reduce_mean.v1\"}\n",
      "upstream = \"*\"\n",
      "\n",
      "[components.transformer]\n",
      "factory = \"transformer\"\n",
      "max_batch_items = 4096\n",
      "set_extra_annotations = {\"@annotation_setters\":\"spacy-transformers.null_annotation_setter.v1\"}\n",
      "\n",
      "[components.transformer.model]\n",
      "@architectures = \"spacy-transformers.TransformerModel.v1\"\n",
      "name = \"roberta-base\"\n",
      "\n",
      "[components.transformer.model.get_spans]\n",
      "@span_getters = \"spacy-transformers.strided_spans.v1\"\n",
      "window = 128\n",
      "stride = 96\n",
      "\n",
      "[components.transformer.model.tokenizer_config]\n",
      "use_fast = true\n",
      "\n",
      "[corpora]\n",
      "\n",
      "[corpora.dev]\n",
      "@readers = \"spacy.Corpus.v1\"\n",
      "path = ${paths.dev}\n",
      "max_length = 0\n",
      "gold_preproc = false\n",
      "limit = 0\n",
      "augmenter = null\n",
      "\n",
      "[corpora.train]\n",
      "@readers = \"spacy.Corpus.v1\"\n",
      "path = ${paths.train}\n",
      "max_length = 500\n",
      "gold_preproc = false\n",
      "limit = 0\n",
      "augmenter = null\n",
      "\n",
      "[training]\n",
      "accumulate_gradient = 3\n",
      "dev_corpus = \"corpora.dev\"\n",
      "train_corpus = \"corpora.train\"\n",
      "seed = ${system.seed}\n",
      "gpu_allocator = ${system.gpu_allocator}\n",
      "dropout = 0.5958824689\n",
      "patience = 1600\n",
      "max_epochs = 0\n",
      "max_steps = 20000\n",
      "eval_frequency = 200\n",
      "frozen_components = []\n",
      "annotating_components = []\n",
      "before_to_disk = null\n",
      "\n",
      "[training.batcher]\n",
      "@batchers = \"spacy.batch_by_padded.v1\"\n",
      "discard_oversize = true\n",
      "size = 2000\n",
      "buffer = 256\n",
      "get_length = null\n",
      "\n",
      "[training.logger]\n",
      "@loggers = \"spacy.WandbLogger.v2\"\n",
      "project_name = \"bib-tuning\"\n",
      "remove_config_values = []\n",
      "model_log_interval = null\n",
      "log_dataset_dir = null\n",
      "\n",
      "[training.optimizer]\n",
      "@optimizers = \"Adam.v1\"\n",
      "beta1 = 0.9\n",
      "beta2 = 0.999\n",
      "L2_is_weight_decay = true\n",
      "L2 = 0.01\n",
      "grad_clip = 1.0\n",
      "use_averages = false\n",
      "eps = 0.00000001\n",
      "\n",
      "[training.optimizer.learn_rate]\n",
      "@schedules = \"warmup_linear.v1\"\n",
      "warmup_steps = 250\n",
      "total_steps = 20000\n",
      "initial_rate = 0.00005\n",
      "\n",
      "[training.score_weights]\n",
      "ents_f = 1.0\n",
      "ents_p = 0.0\n",
      "ents_r = 0.0\n",
      "ents_per_type = null\n",
      "\n",
      "[pretraining]\n",
      "\n",
      "[initialize]\n",
      "vectors = null\n",
      "init_tok2vec = ${paths.init_tok2vec}\n",
      "vocab_data = null\n",
      "lookups = null\n",
      "before_init = null\n",
      "after_init = null\n",
      "\n",
      "[initialize.components]\n",
      "\n",
      "[initialize.tokenizer]\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config ../archive/tuning-config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m‚Ñπ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2022-05-30 20:42:55,789] [INFO] Set up nlp object from config\n",
      "[2022-05-30 20:42:57,628] [INFO] Pipeline: ['transformer', 'ner']\n",
      "[2022-05-30 20:42:57,631] [INFO] Created vocabulary\n",
      "[2022-05-30 20:42:57,632] [INFO] Finished initializing nlp object\n",
      "[2022-05-30 20:43:07,771] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
      "\u001b[38;5;2m‚úî Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Initial learn rate: 0.0\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlafias\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.17\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/slafia/bibliography-mentions/notebooks/wandb/run-20220530_204308-3dojjwrn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msnowy-wind-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/lafias/bib-tuning\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/lafias/bib-tuning/runs/3dojjwrn\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0        3578.14    162.59    0.00    0.00    0.00    0.00\n",
      "  2     200      146864.49  38440.98   30.10   22.61   45.00    0.30\n",
      "  5     400         741.08   1681.13   51.39   37.22   83.00    0.51\n",
      "  7     600         289.54    996.76   52.84   39.70   79.00    0.53\n",
      "  9     800         142.67    907.25   50.50   37.81   76.00    0.50\n",
      " 12    1000         113.63    846.03   53.12   38.64   85.00    0.53\n",
      " 14    1200         106.62    761.28   80.60   80.20   81.00    0.81\n",
      " 17    1400          26.15    654.41   84.73   83.50   86.00    0.85\n",
      " 19    1600          55.99    739.04   82.24   77.19   88.00    0.82\n",
      " 22    1800          49.24    679.02   83.42   83.84   83.00    0.83\n",
      " 24    2000          45.00    627.61   80.21   83.70   77.00    0.80\n",
      " 27    2200          16.10    578.24   83.90   81.90   86.00    0.84\n",
      " 29    2400          26.43    559.12   86.70   85.44   88.00    0.87\n",
      " 32    2600          45.44    563.29   84.62   81.48   88.00    0.85\n",
      " 34    2800          46.31    525.25   84.54   87.23   82.00    0.85\n",
      " 37    3000          41.30    508.68   82.05   84.21   80.00    0.82\n",
      " 39    3200          42.62    464.83   86.63   93.10   81.00    0.87\n",
      " 42    3400           1.82    410.72   84.69   86.46   83.00    0.85\n",
      " 44    3600           1.47    428.48   81.77   80.58   83.00    0.82\n",
      " 47    3800           0.00    363.23   84.69   86.46   83.00    0.85\n",
      " 49    4000          34.30    361.76   80.75   76.11   86.00    0.81\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ents_f ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ents_p ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ents_r ‚ñÅ‚ñÖ‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         loss_ner ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss_transformer ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            score ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            speed ‚ñÖ‚ñà‚ñÑ‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        token_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          token_f ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          token_p ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          token_r ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ents_f 0.80751\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ents_p 0.76106\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           ents_r 0.86\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         loss_ner 361.75779\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss_transformer 34.30066\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            score 0.80751\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            speed 12383.78754\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        token_acc 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          token_f 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          token_p 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          token_r 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msnowy-wind-2\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/lafias/bib-tuning/runs/3dojjwrn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220530_204308-3dojjwrn/logs\u001b[0m\n",
      "\u001b[38;5;2m‚úî Saved pipeline to output directory\u001b[0m\n",
      "../corpus/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ../config.cfg --output ../corpus/ --paths.train ../ner/train.spacy --paths.dev ../ner/dev.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
